{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip install pytube\n",
        "import os\n",
        "# from os.path import exists\n",
        "# if not exists('unbalanced_train_segments.csv'):\n",
        "  # !curl http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/unbalanced_train_segments.csv -o unbalanced_train_segments.csv\n",
        "\n",
        "!gdown 1Syt6DFnTMk0PolN76MtabSrGYj4_FVoL\n",
        "!unzip -q code.zip\n",
        "!wget -O data.zip https://www.dropbox.com/s/oipv2a03ro1l3f0/data_small.zip?dl=0\n",
        "!unzip -q data.zip\n",
        "os.rename('data_small', 'data')"
      ],
      "metadata": {
        "id": "JZf1Nj6q7RaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI_3CLs71ZDE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import notebook\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from dataset import AudioSet\n",
        "import visualize as v\n",
        "from config import get_config\n",
        "cfg = get_config('config.ini')\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = None\n",
        "trainAS = AudioSet('train', cfg['SAVE_DIR'], VAL_RATIO=cfg['VAL_RATIO'], TEST_RATIO=cfg['TEST_RATIO'], FT_RATIO=cfg['FT_RATIO'], size=size)\n",
        "valAS = AudioSet('val', cfg['SAVE_DIR'], VAL_RATIO=cfg['VAL_RATIO'], TEST_RATIO=cfg['TEST_RATIO'], FT_RATIO=cfg['FT_RATIO'], size=size)\n",
        "testAS = AudioSet('test', cfg['SAVE_DIR'], VAL_RATIO=cfg['VAL_RATIO'], TEST_RATIO=cfg['TEST_RATIO'], FT_RATIO=cfg['FT_RATIO'], size=size)"
      ],
      "metadata": {
        "id": "asyd1aptBWOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  idx = np.random.randint(0, len(trainAS))\n",
        "  v.viz_pair(trainAS, idx, t='train pair w augmentation')\n",
        "  v.viz_clip(trainAS, idx, t='train ')\n",
        "for i in range(5):\n",
        "  idx = np.random.randint(0, len(valAS))\n",
        "  v.viz_clip(valAS, idx, t='val ')\n",
        "for i in range(5):\n",
        "  idx = np.random.randint(0, len(testAS))\n",
        "  v.viz_clip(testAS, idx, t='test ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WhILjF0Ywhn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionConvNet, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # conv1_1\n",
        "        nn.Conv2d(3, 64, 3, 2, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # conv1_2\n",
        "        nn.Conv2d(64, 64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # pool1\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv2_1\n",
        "        nn.Conv2d(64, 128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # conv2_2 \n",
        "        nn.Conv2d(128,128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # pool2\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv3_1\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # conv3_2\n",
        "        nn.Conv2d(256, 256, 3, 1, 1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # pool3\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv4_1\n",
        "        nn.Conv2d(256,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(),\n",
        "        # conv4_2\n",
        "        nn.Conv2d(512,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.net(x)\n",
        "    return h\n",
        "\n",
        "class AudioConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AudioConvNet, self).__init__()\n",
        "    \n",
        "    self.net = nn.Sequential(\n",
        "        # conv1_1\n",
        "        nn.Conv2d(1, 64, 3, 2, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # conv1_2\n",
        "        nn.Conv2d(64, 64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # pool1\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv2_1\n",
        "        nn.Conv2d(64, 128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # conv2_2 \n",
        "        nn.Conv2d(128,128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # pool2\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv3_1\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # conv3_2\n",
        "        nn.Conv2d(256, 256, 3, 1, 1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # pool3\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv4_1\n",
        "        nn.Conv2d(256,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(),\n",
        "        # conv4_2\n",
        "        nn.Conv2d(512,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.net(x)\n",
        "    return h\n",
        "\n",
        "class AVOLNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AVOLNet, self).__init__()\n",
        "\n",
        "    # image network\n",
        "    self.visionNet = VisionConvNet()\n",
        "    self.conv5 = nn.Conv2d(512, 128, 1)\n",
        "    self.conv6 = nn.Conv2d(128, 128, 1)\n",
        "\n",
        "    # audio network\n",
        "    self.audioNet = AudioConvNet()\n",
        "    self.pool4 = nn.AdaptiveMaxPool2d(1)\n",
        "    self.fc1 = nn.Linear(512,128)\n",
        "    self.fc2 = nn.Linear(128,128)\n",
        "\n",
        "    # fusion network\n",
        "    self.conv7 = nn.Conv2d(1,1,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "    self.visionNet.norm_init(mean, std)\n",
        "    self.audioNet.norm_init(mean, std)\n",
        "\n",
        "  def forward(self, image, audio):\n",
        "\n",
        "    # process image\n",
        "    img = self.visionNet(image)\n",
        "    img = self.conv5(img)\n",
        "    img = self.conv6(img)\n",
        "    # img = torch.reshape(img,(img.shape[0],-1,128))\n",
        "    img = torch.permute(torch.reshape(img,(img.shape[0],128,-1)), (0,2,1))\n",
        "\n",
        "\n",
        "    # process audio\n",
        "    aud = self.audioNet(audio)\n",
        "    aud = self.pool4(aud)\n",
        "    aud = aud.squeeze(2).squeeze(2)\n",
        "    aud = F.relu(self.fc1(aud))\n",
        "    aud = self.fc2(aud)\n",
        "    aud = torch.reshape(aud, (aud.shape[0], 128, -1))\n",
        "\n",
        "    # fuse embeddings\n",
        "    dot_product = img @ aud\n",
        "    dot_product = torch.reshape(dot_product, (dot_product.shape[0],1,8,8))\n",
        "\n",
        "    localisation = self.conv7(dot_product)\n",
        "    localisation = self.sigmoid(localisation)\n",
        "    corresponds = self.maxpool(localisation).squeeze()\n",
        "\n",
        "    return corresponds, localisation.squeeze(1)"
      ],
      "metadata": {
        "id": "_dQybPsz1jpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_AVOL(model, viz_loader, train_loader, val_loader, learning_rate = 5e-4, weight_decay = 1e-5, epochs = 200, device = \"cpu\", start_epoch = 0):\n",
        "  \n",
        "  # reduce learning rate if starting epoch is not zero\n",
        "  learning_rate = 0.94**(start_epoch // 16)* learning_rate\n",
        "\n",
        "  # initialise optimizer and lr scheduler\n",
        "  optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  lr_scheduler = optim.lr_scheduler.StepLR(optimizer,16,0.94) # reduces learning rate by 6% every 16 epochs\n",
        "\n",
        "  # initialise loss criterion\n",
        "  criterion = nn.BCELoss()\n",
        "\n",
        "  # Keep track of loss for plotting.\n",
        "  train_loss_history = []\n",
        "  train_acc_history = []\n",
        "  eval_loss_history = []\n",
        "  eval_acc_history = []\n",
        "\n",
        "  up = nn.Upsample(scale_factor=16)\n",
        "\n",
        "  tp = {'left':False, 'right':False, 'labelleft':False, 'labelbottom':False, 'bottom':False}\n",
        "  match_str = ['fake pair','real pair']\n",
        "  for epoch in notebook.tqdm(range(start_epoch, epochs)):\n",
        "    train_loss_epoch = 0.0\n",
        "    train_acc_epoch = 0.0\n",
        "\n",
        "    # train model \n",
        "    model.train()\n",
        "\n",
        "    batch_num = 0\n",
        "    for images, audios, labels, indx in train_loader:\n",
        "      print('\\rbatch: ' + str(batch_num), end='')\n",
        "      batch_num += 1\n",
        "\n",
        "      # send to device\n",
        "      images, audios, labels = images.to(device), audios.to(device, dtype=torch.float), labels.to(device).squeeze()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output, local = model(images, audios)\n",
        "      \n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        train_predict = torch.round(output)\n",
        "    \n",
        "      train_loss_epoch += loss.detach()\n",
        "      train_acc_epoch += torch.sum(train_predict == labels).detach()\n",
        "    \n",
        "    if epoch % 1 == 0:\n",
        "      it = iter(viz_loader)\n",
        "      img,aud,lab,indx = next(it)\n",
        "      img=img.to(device, dtype=torch.float)\n",
        "      aud=aud.to(device, dtype=torch.float)\n",
        "      output, local = model(img, aud)\n",
        "      N = len(img)\n",
        "      L = np.array(local.detach().cpu())\n",
        "      loc = local.reshape(local.shape[0], 1, local.shape[1], local.shape[2])\n",
        "      loc = loc.expand(loc.shape[0], 3, loc.shape[2], loc.shape[3])\n",
        "      loc = up(loc)\n",
        "      fig,ax = plt.subplots(nrows=2, ncols=N, figsize=(2*N, 4))\n",
        "      img = np.transpose(np.array((img + loc).detach().cpu()), axes=[0,2,3,1])\n",
        "      aud = np.squeeze(np.array(aud.detach().cpu()))\n",
        "      for i in range(N):\n",
        "        print('\\r{}/{} visualisations'.format(i,N-1), end='')\n",
        "        maxi = np.argmax(L[i])\n",
        "        mini = np.argmin(L[i])\n",
        "        img[i] -= np.min(img[i])\n",
        "        ax[0][i].imshow(img[i]/np.max(img[i]))\n",
        "        ax[1][i].imshow(aud[i])\n",
        "        ax[0][i].set_title('{},{} {:.3f}\\n{},{} {:.3f}'\\\n",
        "                          .format(mini//L.shape[1], mini%L.shape[1], np.min(L[i]),\\\n",
        "                                  maxi//L.shape[1], maxi%L.shape[1], np.max(L[i])), y=0, pad=7)\n",
        "        ax[1][i].set_title('{} {:.0f} {:.0f} {:.0f} {:.0f}'.format(match_str[int(lab[i].item())], *list(indx[i])), y=0, pad=7)\n",
        "        ax[0][i].tick_params('both', **tp)\n",
        "        ax[1][i].tick_params('both', **tp)\n",
        "\n",
        "      fig.subplots_adjust(left=0, right=1, bottom=0, top=1, wspace=0, hspace=0)\n",
        "      plt.savefig('save/epoch {} localisation.jpg'.format(epoch), dpi=300)\n",
        "      plt.close(fig)\n",
        "\n",
        "      img,aud,lab,indx = next(it)\n",
        "      img=img.to(device, dtype=torch.float).requires_grad_(True)\n",
        "      aud=aud.to(device, dtype=torch.float).requires_grad_(True)\n",
        "      output, local = model(img,aud)\n",
        "      local = torch.sum(local, dim=0)\n",
        "      s = tuple(local.shape)\n",
        "      fig,ax = plt.subplots(nrows=s[0], ncols=s[1], figsize=(s[0]*2, s[1]*2))\n",
        "      for i in range(s[0]):\n",
        "        for j in range(s[1]):\n",
        "          print('\\r{},{} RoIs'.format(i,j), end='')\n",
        "          local[i,j].backward(retain_graph=True)\n",
        "          grads = torch.sum(img.grad, dim=0)\n",
        "          img.grad = None\n",
        "          grads = np.transpose(grads.detach().cpu().numpy(), axes=[1,2,0])\n",
        "          m = np.min(grads)\n",
        "          M = np.max(grads)\n",
        "          grads -= m\n",
        "          grads /= np.max(grads)\n",
        "          ax[i][j].imshow(grads)\n",
        "          ax[i][j].set_title('{:.3f} to {:.3f}'.format(m, M), y=0, pad=7)\n",
        "          ax[i][j].tick_params('both', **tp)\n",
        "\n",
        "      fig.subplots_adjust(left=0, right=1, bottom=0, top=1, wspace=0, hspace=0)\n",
        "      plt.savefig('save/epoch {} RoI.jpg'.format(epoch), dpi=300)\n",
        "      plt.close(fig)\n",
        "\n",
        "    train_loss_history.append(train_loss_epoch)\n",
        "    train_acc_epoch /= len(train_loader.dataset)\n",
        "    train_acc_history.append(train_acc_epoch)\n",
        "    \n",
        "    # evaluate model \n",
        "    eval_loss_epoch = 0.0\n",
        "    eval_acc_epoch = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    for images, audios, labels, _ in val_loader:\n",
        "      # send to device \n",
        "      images, audios, labels = images.to(device), audios.to(device, dtype=torch.float), labels.to(device).squeeze()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        output, _ = model(images, audios)\n",
        "        loss = criterion(output, labels)\n",
        "        eval_predict = torch.round(output)\n",
        "\n",
        "      eval_loss_epoch += loss.detach()\n",
        "      eval_acc_epoch += torch.sum(eval_predict == labels).detach()\n",
        "\n",
        "    eval_loss_history.append(eval_loss_epoch)\n",
        "    eval_acc_epoch /= len(val_loader.dataset)\n",
        "    eval_acc_history.append(eval_acc_epoch)\n",
        "\n",
        "    # step scheduler at end of epoch\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # print loss values\n",
        "    #if epoch % 10 == 0:\n",
        "    if True:\n",
        "      # Open filestream for saving log\n",
        "      fileOut = open('save/training_log.txt', 'a')\n",
        "      print('\\r', end='')\n",
        "      print('Epoch {}, training loss {:.3f}, train accuracy {}, validation loss {:.3f}, validation accuracy {}'.format(epoch, train_loss_epoch, train_acc_epoch, eval_loss_epoch, eval_acc_epoch))\n",
        "      print('Epoch {}, training loss {:.3f}, train accuracy {}, validation loss {:.3f}, validation accuracy {}'.format(epoch, train_loss_epoch, train_acc_epoch, eval_loss_epoch, eval_acc_epoch),file=fileOut)\n",
        "      torch.save(model.state_dict(), \"save/AVOLNet{}.pt\".format(epoch))\n",
        "      fileOut.close()\n",
        "\n",
        "\n",
        "  return train_loss_history, train_acc_history, eval_loss_history, eval_acc_history\n",
        "\n",
        "def test_AVOL(model, test_loader, show_location = False):\n",
        "  test_acc_epoch = 0\n",
        "\n",
        "  model.eval()\n",
        "  for images, audios, labels in test_loader:\n",
        "    # send to device \n",
        "    images, audio, labels = images.to(device), audio.to(device), labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output, location = model(images, audio)\n",
        "      test_predict = torch.round(output)\n",
        "      test_acc_epoch += torch.sum(test_predict == labels)\n",
        "      if (show_location):\n",
        "        # 1 for true pair, 0 for false pair\n",
        "        imgs = images[labels == 1 and test_predict == 1]\n",
        "        locs = location[labels == 1 and test_predict == 1]\n",
        "        for img, loc in zip(imgs,locs):\n",
        "          output_img = visualize_localization(img,loc)\n",
        "          plt.imshow(output_img)\n",
        "  \n",
        "  test_acc_epoch /= len(test_loader.dataset)\n",
        "\n",
        "  print('Test accuracy {}'.format(test_acc_epoch))\n"
      ],
      "metadata": {
        "id": "Bl-YunKRXiZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_localization(image, loc):\n",
        "  # inverse tranformation (may not be needed)\n",
        "\n",
        "  inverse_norm = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
        "            transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1.0, 1.0, 1.0])])\n",
        "\n",
        "  img = inverse_norm(image)\n",
        "\n",
        "  # convert from RGB to BGR \n",
        "  new_img = torch.clone(img)\n",
        "  new_img[0, :, :] = img[2, :, :]\n",
        "  new_img[2, :, :] = img[0, :, :]\n",
        "  img = new_img.cpu()\n",
        "\n",
        "  # convert from (C x H x W) to (H x W x C)\n",
        "  img = img.numpy().transpose((1, 2, 0))\n",
        "  img = (img/img.max())*255\n",
        "  img = img.astype(np.uint8)\n",
        "\n",
        "  # convert location map into image\n",
        "  loc = loc.cpu()\n",
        "  loc = (loc * 255).numpy().astype(np.uint8)\n",
        "  heatmap = cv2.applyColorMap(loc, cv2.COLORMAP_HOT)\n",
        "\n",
        "  # resize heatmap to be same with img\n",
        "  heatmap_upscaled = cv2.resize(heatmap, (img.shape[0], img.shape[1]), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "  # combine heatmap with image\n",
        "  dst = cv2.addWeighted(img, 0.5, heatmap_upscaled, 0.5, 0)\n",
        "\n",
        "  return cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "zDp2j0YFKN96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train network \n",
        "\n",
        "# TO DO: \n",
        "train = trainAS\n",
        "eval = valAS\n",
        "train_loader = DataLoader(train,batch_size=64,shuffle=True,num_workers=2,pin_memory=True)\n",
        "val_loader = DataLoader(eval,batch_size=64,shuffle=True,num_workers=2,pin_memory=True)\n",
        "\n",
        "AVOL = AVOLNet().to(device)\n",
        "\n",
        "# load existing weights (OPTIONAL)\n",
        "weights_path = None\n",
        "if weights_path is not None:\n",
        "  AVOL.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
        "\n",
        "tr_loss, tr_acc, val_losas, val_acc = train_AVOL(AVOL, train_loader, val_loader, learning_rate = 5e-4, epochs=100,device=device, start_epoch=0)\n",
        "\n",
        "# visualize losses\n",
        "plt.title(\"Training loss history\")\n",
        "plt.xlabel(f\"Iteration (x 10)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(tr_loss.cpu())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QGnOzUr0GSuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test network\n",
        "\n",
        "test = testAS\n",
        "\n",
        "test_loader = DataLoader(test,batch_size=50,shuffle=True,num_workers=2,pin_memory=True)\n",
        "\n",
        "AVOL = AVOLNet().to(device)\n",
        "\n",
        "weights_path = \"AVOLNet100.pt\"\n",
        "# if weights_path is not None:\n",
        "AVOL.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
        "\n",
        "test_AVOL(AVOL,test_loader, show_location=True)"
      ],
      "metadata": {
        "id": "gGzi1jOngqtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save network weights\n",
        "torch.save(AVOL.state_dict(), \"AVOLNet.pt\")"
      ],
      "metadata": {
        "id": "sn1AQb8BHVUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load network weights\n",
        "AVOL = AVOLNet().to(device)\n",
        "AVOL.load_state_dict(torch.load(\"AVOLNet.pt\", map_location=\"cpu\"))"
      ],
      "metadata": {
        "id": "0Czty0pqHbZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize receptive field\n",
        "AOEloader = DataLoader(train,batch_size=8,shuffle=False,num_workers=2,pin_memory=True,sampler=np.random.permutation(len(trainAS)))\n",
        "it = iter(AOEloader)\n",
        "img,aud,lab,indx = next(it)\n",
        "img=img.to(device, dtype=torch.float).requires_grad_(True)\n",
        "aud=aud.to(device, dtype=torch.float).requires_grad_(True)\n",
        "# newAVOL = AVOLNet().to(device)\n",
        "# newAVOL.norm_init(0, 1e-3)\n",
        "output, local = AVOL(img,aud)\n",
        "local = torch.sum(local, dim=0)\n",
        "s = tuple(local.shape)\n",
        "print(s)\n",
        "fig,ax = plt.subplots(nrows=s[0], ncols=s[1], figsize=(s[0]*2, s[1]*2))\n",
        "tp = {'left':False, 'right':False, 'labelleft':False, 'labelbottom':False, 'bottom':False}\n",
        "for i in range(s[0]):\n",
        "  for j in range(s[1]):\n",
        "    print('\\r{},{}'.format(i,j), end='')\n",
        "    local[i,j].backward(retain_graph=True)\n",
        "    grads = torch.sum(img.grad, dim=0)\n",
        "    img.grad = None\n",
        "    grads = np.transpose(grads.detach().cpu().numpy(), axes=[1,2,0])\n",
        "    m = np.min(grads)\n",
        "    M = np.max(grads)\n",
        "    grads -= m\n",
        "    grads /= np.max(grads)\n",
        "    ax[i][j].imshow(grads)\n",
        "    ax[i][j].set_title('{:.3f} to {:.3f}'.format(m, M), y=0, pad=7)\n",
        "    ax[i][j].tick_params('both', **tp)\n",
        "\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, wspace=0, hspace=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PkMNPX0OhTqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#debugging visualize receptive field.\n",
        "class newAVOLNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(newAVOLNet, self).__init__()\n",
        "\n",
        "    # image network\n",
        "    self.visionNet = VisionConvNet()\n",
        "    self.conv5 = nn.Conv2d(512, 128, 1)\n",
        "    self.conv6 = nn.Conv2d(128, 128, 1)\n",
        "\n",
        "    # audio network\n",
        "    self.audioNet = AudioConvNet()\n",
        "    self.pool4 = nn.AdaptiveMaxPool2d(1)\n",
        "    self.fc1 = nn.Linear(512,128)\n",
        "    self.fc2 = nn.Linear(128,128)\n",
        "\n",
        "    # fusion network\n",
        "    self.conv7 = nn.Conv2d(1,1,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "    self.visionNet.norm_init(mean, std)\n",
        "    self.audioNet.norm_init(mean, std)\n",
        "\n",
        "  def forward(self, image, audio):\n",
        "\n",
        "    # process image\n",
        "    img = self.visionNet(image)\n",
        "    img = self.conv5(img)\n",
        "    img = self.conv6(img)\n",
        "    img = torch.permute(torch.reshape(img,(img.shape[0],128,-1)), (0,2,1))\n",
        "\n",
        "\n",
        "    # process audio\n",
        "    aud = self.audioNet(audio)\n",
        "    aud = self.pool4(aud)\n",
        "    aud = aud.squeeze(2).squeeze(2)\n",
        "    aud = F.relu(self.fc1(aud))\n",
        "    aud = self.fc2(aud)\n",
        "    aud = torch.reshape(aud, (aud.shape[0], 128, -1))\n",
        "\n",
        "    # fuse embeddings\n",
        "    dot_product = img @ aud\n",
        "    dot_product = torch.reshape(dot_product, (dot_product.shape[0],1,8,8))\n",
        "\n",
        "    localisation = self.conv7(dot_product)\n",
        "    localisation = self.sigmoid(localisation)\n",
        "    corresponds = self.maxpool(localisation).squeeze()\n",
        "\n",
        "    return corresponds, localisation.squeeze(1)\n",
        "\n",
        "AOEloader = DataLoader(trainAS,batch_size=8,shuffle=False,num_workers=2,pin_memory=True,sampler=np.random.permutation(len(trainAS)))\n",
        "it = iter(AOEloader)\n",
        "img,aud,lab,indx = next(it)\n",
        "img=img.to(device, dtype=torch.float).requires_grad_(True)\n",
        "aud=aud.to(device, dtype=torch.float).requires_grad_(True)\n",
        "newAVOL = newAVOLNet().to(device)\n",
        "newAVOL.norm_init(0, 1e-3)\n",
        "output, local = newAVOL(img,aud)\n",
        "local = torch.sum(local, dim=0)\n",
        "s = tuple(local.shape)\n",
        "print(s)\n",
        "fig,ax = plt.subplots(nrows=s[0], ncols=s[1], figsize=(s[0]*2, s[1]*2))\n",
        "tp = {'left':False, 'right':False, 'labelleft':False, 'labelbottom':False, 'bottom':False}\n",
        "for i in range(s[0]):\n",
        "  for j in range(s[1]):\n",
        "    print('\\r{},{}'.format(i,j), end='')\n",
        "    local[i,j].backward(retain_graph=True)\n",
        "    grads = torch.sum(img.grad, dim=0)\n",
        "    img.grad = None\n",
        "    grads = np.transpose(grads.detach().cpu().numpy(), axes=[1,2,0])\n",
        "    m = np.min(grads)\n",
        "    M = np.max(grads)\n",
        "    grads -= m\n",
        "    grads /= np.max(grads)\n",
        "    ax[i][j].imshow(grads)\n",
        "    ax[i][j].set_title('{:.3f} to {:.3f}'.format(m, M), y=0, pad=7)\n",
        "    ax[i][j].tick_params('both', **tp)\n",
        "\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, wspace=0, hspace=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2KtTFN7rhbfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reduced channel netowrk\n",
        "\n",
        "class VisionConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionConvNet, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # conv1_1\n",
        "        nn.Conv2d(3, 32, 3, 2, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        # conv1_2\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        # pool1\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv2_1\n",
        "        nn.Conv2d(32, 64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # conv2_2 \n",
        "        nn.Conv2d(64, 64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # pool2\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv3_1\n",
        "        nn.Conv2d(64,128,3,1,1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # conv3_2\n",
        "        nn.Conv2d(128, 128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # pool3\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv4_1\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # conv4_2\n",
        "        nn.Conv2d(256,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.net(x)\n",
        "    return h\n",
        "\n",
        "class AudioConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AudioConvNet, self).__init__()\n",
        "    \n",
        "    self.net = nn.Sequential(\n",
        "        # conv1_1\n",
        "        nn.Conv2d(1, 32, 3, 2, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        # conv1_2\n",
        "        nn.Conv2d(32, 32, 3, 1, 1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        # pool1\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv2_1\n",
        "        nn.Conv2d(32, 64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # conv2_2 \n",
        "        nn.Conv2d(64,64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # pool2\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv3_1\n",
        "        nn.Conv2d(64,128,3,1,1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # conv3_2\n",
        "        nn.Conv2d(128, 128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # pool3\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv4_1\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # conv4_2\n",
        "        nn.Conv2d(256,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.net(x)\n",
        "    return h\n",
        "\n",
        "class AVOLNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AVOLNet, self).__init__()\n",
        "\n",
        "    # image network\n",
        "    self.visionNet = VisionConvNet()\n",
        "    self.conv5 = nn.Conv2d(256, 64, 1)\n",
        "    self.conv6 = nn.Conv2d(64, 64, 1)\n",
        "\n",
        "    # audio network\n",
        "    self.audioNet = AudioConvNet()\n",
        "    self.pool4 = nn.AdaptiveMaxPool2d(1)\n",
        "    self.fc1 = nn.Linear(256,64)\n",
        "    self.fc2 = nn.Linear(64,64)\n",
        "\n",
        "    # fusion network\n",
        "    self.conv7 = nn.Conv2d(1,1,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "    self.visionNet.norm_init(mean, std)\n",
        "    self.audioNet.norm_init(mean, std)\n",
        "\n",
        "  def forward(self, image, audio):\n",
        "\n",
        "    # process image\n",
        "    img = self.visionNet(image)\n",
        "    img = self.conv5(img)\n",
        "    img = self.conv6(img)\n",
        "    img = torch.reshape(img,(img.shape[0],64,-1))\n",
        "    img = torch.permute(img,(0,2,1))\n",
        "\n",
        "    # process audio\n",
        "    aud = self.audioNet(audio)\n",
        "    aud = self.pool4(aud)\n",
        "    aud = aud.squeeze(2).squeeze(2)\n",
        "    aud = F.relu(self.fc1(aud))\n",
        "    aud = self.fc2(aud)\n",
        "    aud = torch.reshape(aud, (aud.shape[0], 64, -1))\n",
        "\n",
        "    # fuse embeddings\n",
        "    dot_product = img @ aud\n",
        "    dot_product = torch.reshape(dot_product, (dot_product.shape[0],1,8,8))\n",
        "\n",
        "    localisation = self.conv7(dot_product)\n",
        "    localisation = self.sigmoid(localisation)\n",
        "    corresponds = self.maxpool(localisation).squeeze()\n",
        "\n",
        "    return corresponds, localisation.squeeze(1)"
      ],
      "metadata": {
        "id": "qhBx68FsMFxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reduced layers network\n",
        "class VisionConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VisionConvNet, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # conv1_1\n",
        "        nn.Conv2d(3, 64, 3, 2, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv2_1\n",
        "        nn.Conv2d(64, 128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # pool2\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv3_1\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # pool3\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv4_1\n",
        "        nn.Conv2d(256,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(),\n",
        "        # conv4_2\n",
        "        nn.Conv2d(512,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.net(x)\n",
        "    return h\n",
        "\n",
        "class AudioConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AudioConvNet, self).__init__()\n",
        "    \n",
        "    self.net = nn.Sequential(\n",
        "        # conv1_1\n",
        "        nn.Conv2d(1, 64, 3, 2, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # conv1_2\n",
        "        nn.Conv2d(64, 64, 3, 1, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # pool1\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv2_1\n",
        "        nn.Conv2d(64, 128, 3, 1, 1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        # conv2_2 \n",
        "        # pool2\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv3_1\n",
        "        nn.Conv2d(128,256,3,1,1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # pool3\n",
        "        nn.MaxPool2d(2),\n",
        "        # conv4_1\n",
        "        nn.Conv2d(256,512,3,1,1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(),\n",
        "        # conv4_2\n",
        "    )\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h = self.net(x)\n",
        "    return h\n",
        "\n",
        "class AVOLNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AVOLNet, self).__init__()\n",
        "\n",
        "    # image network\n",
        "    self.visionNet = VisionConvNet()\n",
        "    self.conv5 = nn.Conv2d(512, 128, 1)\n",
        "    self.conv6 = nn.Conv2d(128, 128, 1)\n",
        "\n",
        "    # audio network\n",
        "    self.audioNet = AudioConvNet()\n",
        "    self.pool4 = nn.AdaptiveMaxPool2d(1)\n",
        "    self.fc1 = nn.Linear(512,128)\n",
        "    self.fc2 = nn.Linear(128,128)\n",
        "\n",
        "    # fusion network\n",
        "    self.conv7 = nn.Conv2d(1,1,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
        "    \n",
        "  def norm_init(self, mean, std):\n",
        "    for m in self._modules:\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "          m.weight.data.normal_(mean, std)\n",
        "          m.bias.data.zero_()\n",
        "    self.visionNet.norm_init(mean, std)\n",
        "    self.audioNet.norm_init(mean, std)\n",
        "\n",
        "  def forward(self, image, audio):\n",
        "\n",
        "    # process image\n",
        "    img = self.visionNet(image)\n",
        "    img = self.conv5(img)\n",
        "    img = self.conv6(img)\n",
        "    # img = torch.reshape(img,(img.shape[0],-1,128))\n",
        "    img = torch.permute(torch.reshape(img,(img.shape[0],128,-1)), (0,2,1))\n",
        "\n",
        "\n",
        "    # process audio\n",
        "    aud = self.audioNet(audio)\n",
        "    aud = self.pool4(aud)\n",
        "    aud = aud.squeeze(2).squeeze(2)\n",
        "    aud = F.relu(self.fc1(aud))\n",
        "    aud = self.fc2(aud)\n",
        "    aud = torch.reshape(aud, (aud.shape[0], 128, -1))\n",
        "\n",
        "    # fuse embeddings\n",
        "    dot_product = img @ aud\n",
        "    dot_product = torch.reshape(dot_product, (dot_product.shape[0],1,8,8))\n",
        "\n",
        "    localisation = self.conv7(dot_product)\n",
        "    localisation = self.sigmoid(localisation)\n",
        "    corresponds = self.maxpool(localisation).squeeze()\n",
        "\n",
        "    return corresponds, localisation.squeeze(1)"
      ],
      "metadata": {
        "id": "mmpx8V2Iho8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}